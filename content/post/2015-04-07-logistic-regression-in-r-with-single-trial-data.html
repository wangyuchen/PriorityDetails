---
title: ' Logistic Regression in R with Single-Trial Data'
author: Yuchen Wang
date: '2015-04-07'
slug: logistic-regression-in-r-with-single-trial-data
categories:
  - R
  - Statistics
tags:
  - logistic regression
  - glm
  - regression
---



<p>The examples in this post is taken from a class I took at the University of Missouri. These examples are originally provided in SAS and I translated them to R.</p>
<hr />
<p>The relationship between countries’ credit ratings and the volatility of the countries’ stock markets was examined in the <em>Journal of Portfolio Management</em> (Spring 1996). Our interest lies in whether volatility (standard deviation of stock returns) and credit rating (in percent) can be used to classify a country into one of the two market types (developed or emerging).</p>
<div id="descriptive-analysis" class="section level2">
<h2>Descriptive analysis</h2>
<p>The data is read into a <code>data.frame</code> with the response as a factor. It’s customary and preferable to use a classification variable as a factor and let R do the rest of the things. When the factor is created from character data like we do in this example, the default order of the levels of that factor are alphabetical. It can be changed so that the level in higher order are coded with larger number. In <code>glm()</code>, it always model the level with higher order as the event. Therefore here we changed the order of levels so that in glm we can model the probability of the response being a developed country.</p>
<pre class="r"><code>volatile &lt;- read.table(&quot;datasets/volatile.dat&quot;)
names(volatile) &lt;- c(&#39;country/region&#39;, &#39;volatile&#39;, &#39;credit&#39;, &#39;market&#39;)
volatile$market &lt;- factor(volatile$market, levels = c(&#39;emerge&#39;, &#39;develop&#39;))
kable(volatile)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">country/region</th>
<th align="right">volatile</th>
<th align="right">credit</th>
<th align="left">market</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Afghanistan</td>
<td align="right">55.7</td>
<td align="right">8.3</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Australia</td>
<td align="right">23.9</td>
<td align="right">71.2</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">China</td>
<td align="right">27.2</td>
<td align="right">57.0</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Cuba</td>
<td align="right">55.0</td>
<td align="right">8.7</td>
<td align="left">emerge</td>
</tr>
<tr class="odd">
<td align="left">Germany</td>
<td align="right">20.3</td>
<td align="right">90.9</td>
<td align="left">develop</td>
</tr>
<tr class="even">
<td align="left">France</td>
<td align="right">20.6</td>
<td align="right">89.1</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">India</td>
<td align="right">30.3</td>
<td align="right">46.1</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Belgium</td>
<td align="right">22.3</td>
<td align="right">79.2</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Canada</td>
<td align="right">22.1</td>
<td align="right">80.3</td>
<td align="left">develop</td>
</tr>
<tr class="even">
<td align="left">Ethiopia</td>
<td align="right">47.9</td>
<td align="right">14.1</td>
<td align="left">emerge</td>
</tr>
<tr class="odd">
<td align="left">Haiti</td>
<td align="right">54.9</td>
<td align="right">8.8</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Japan</td>
<td align="right">20.2</td>
<td align="right">91.6</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Libya</td>
<td align="right">36.7</td>
<td align="right">30.0</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Malaysia</td>
<td align="right">24.3</td>
<td align="right">69.1</td>
<td align="left">emerge</td>
</tr>
<tr class="odd">
<td align="left">Mexico</td>
<td align="right">31.8</td>
<td align="right">41.8</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">NewZealand</td>
<td align="right">24.3</td>
<td align="right">69.4</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Nigeria</td>
<td align="right">46.2</td>
<td align="right">15.8</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Oman</td>
<td align="right">28.6</td>
<td align="right">51.8</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Panama</td>
<td align="right">38.6</td>
<td align="right">26.4</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Spain</td>
<td align="right">23.4</td>
<td align="right">73.7</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Sudan</td>
<td align="right">60.5</td>
<td align="right">6.0</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Taiwan</td>
<td align="right">22.2</td>
<td align="right">79.9</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Norway</td>
<td align="right">21.4</td>
<td align="right">84.6</td>
<td align="left">develop</td>
</tr>
<tr class="even">
<td align="left">Sweden</td>
<td align="right">23.3</td>
<td align="right">74.1</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Togo</td>
<td align="right">45.1</td>
<td align="right">17.0</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Ukraine</td>
<td align="right">46.3</td>
<td align="right">15.7</td>
<td align="left">emerge</td>
</tr>
<tr class="odd">
<td align="left">UnitedKingdom</td>
<td align="right">20.8</td>
<td align="right">87.8</td>
<td align="left">develop</td>
</tr>
<tr class="even">
<td align="left">UnitedStates</td>
<td align="right">20.3</td>
<td align="right">90.7</td>
<td align="left">develop</td>
</tr>
<tr class="odd">
<td align="left">Vietnam</td>
<td align="right">36.9</td>
<td align="right">29.5</td>
<td align="left">emerge</td>
</tr>
<tr class="even">
<td align="left">Zimbabwe</td>
<td align="right">36.2</td>
<td align="right">31.0</td>
<td align="left">emerge</td>
</tr>
</tbody>
</table>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<p>In R, logistic regression model is fitted by <code>glm()</code> with binomial distribution family. The default link function for binomial family is the logit link.</p>
<pre class="r"><code>fit &lt;- glm(market ~ volatile + credit, family = &quot;binomial&quot;, data = volatile)
fit</code></pre>
<pre><code>## 
## Call:  glm(formula = market ~ volatile + credit, family = &quot;binomial&quot;, 
##     data = volatile)
## 
## Coefficients:
## (Intercept)     volatile       credit  
##   -11.56273      0.04501      0.17385  
## 
## Degrees of Freedom: 29 Total (i.e. Null);  27 Residual
## Null Deviance:       41.46 
## Residual Deviance: 9.254     AIC: 15.25</code></pre>
<div id="model-fit-results" class="section level3">
<h3>Model fit results</h3>
<pre class="r"><code># response profile
summary(volatile$market)</code></pre>
<pre><code>##  emerge develop 
##      16      14</code></pre>
<p>In order to compute the model fit statistics for an intercept only model, we can just fit the model explicitly and use the generic functions.</p>
<pre class="r"><code>fit2 &lt;- glm(market ~ 1, family = &quot;binomial&quot;, data = volatile)
AIC(fit2, fit)</code></pre>
<pre><code>##      df      AIC
## fit2  1 43.45540
## fit   3 15.25401</code></pre>
<pre class="r"><code>BIC(fit2, fit)</code></pre>
<pre><code>##      df      BIC
## fit2  1 44.85660
## fit   3 19.45761</code></pre>
<pre class="r"><code>-2 * logLik(fit2)</code></pre>
<pre><code>## &#39;log Lik.&#39; 41.4554 (df=1)</code></pre>
<pre class="r"><code>-2 * logLik(fit)</code></pre>
<pre><code>## &#39;log Lik.&#39; 9.254014 (df=3)</code></pre>
<p>In the SAS output, the same model is fitted using <code>PROC GENMOD</code>. There are some additional output accessing goodness of fit. Here we compute some of the statistics.</p>
<pre class="r"><code># Deviance
fit$deviance</code></pre>
<pre><code>## [1] 9.254014</code></pre>
<pre class="r"><code>fit$df.residual</code></pre>
<pre><code>## [1] 27</code></pre>
<pre class="r"><code>fit$deviance / fit$df.residual</code></pre>
<pre><code>## [1] 0.3427413</code></pre>
<pre class="r"><code># Pearson&#39;s Chi-Square
sum(residuals(fit, &quot;pearson&quot;)^2)</code></pre>
<pre><code>## [1] 9.81283</code></pre>
<pre class="r"><code>sum(residuals(fit, &quot;pearson&quot;)^2) / fit$df.residual</code></pre>
<pre><code>## [1] 0.3634381</code></pre>
<p>These all show evidence of under-dispersion.</p>
</div>
<div id="hypothesis-testing" class="section level3">
<h3>Hypothesis testing</h3>
<p>We know the likelihood ratio test is just the <span class="math inline">\(-2log\Lambda\)</span>, we can use the information above to calculate it.</p>
<pre class="r"><code>-2 * as.numeric(logLik(fit2) - logLik(fit))</code></pre>
<pre><code>## [1] 32.20138</code></pre>
<pre class="r"><code>1 - pchisq(-2 * as.numeric(logLik(fit2) - logLik(fit)), df = 2)  # p-value</code></pre>
<pre><code>## [1] 1.017556e-07</code></pre>
<p>For Wald test <span class="math inline">\(H_0: L\theta = 0\)</span>, we need to specify L to compute the test statistic. For example if we want to test <span class="math inline">\(H_0: \beta_1 = \beta_2 = 0\)</span>, we can set L in the following way:</p>
<pre class="r"><code>L &lt;- rbind(c(0, 1, 0), c(0, 0, 1))
L</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    0    1    0
## [2,]    0    0    1</code></pre>
<p>There is a function <code>wald.test</code> from the <em>aod</em> package. Note that for a Wald test, we need the variance of the parameter estimates, which is <span class="math inline">\(a(\phi)(\hat{F}\hat{V}\hat{F})^{-1}\)</span>. This matrix can be calculated using the <code>vcov()</code> function. It’s calculated by QR decomposition, so the result is slightly different than those from SAS.</p>
<pre class="r"><code>library(aod)
wald.test(vcov(fit), b = coef(fit), L = L)</code></pre>
<pre><code>## Wald test:
## ----------
## 
## Chi-squared test:
## X2 = 6.0, df = 2, P(&gt; X2) = 0.049</code></pre>
<p>In SAS, the analysis of maximum likelihood estimates is done by Wald test and a Chi-square distribution with degree of freedom 1. This is the same as the a Z-test with a standard normal distribution. The Z statistic is the square root of the Wald Chi-square statistic and the p-values are the same.</p>
</div>
<div id="concordance" class="section level3">
<h3>Concordance</h3>
<p>The concordant and discordant percent all requires the fitted value to be calculated. Note that when we are modeling the probability for <code>develop</code> because it is the second level in the variable <code>market</code>.</p>
<pre class="r"><code>i &lt;- volatile$market == &#39;develop&#39;
j &lt;- volatile$market == &#39;emerge&#39;
pairs &lt;- sum(i) * sum(j)

# expand pairs into grid
grid &lt;- with(volatile, 
             cbind(expand.grid(yi = market[i], yj = market[j]),
                   expand.grid(pi = fitted(fit)[i], pj = fitted(fit)[j])))

# Percent Concordant
sum(grid$pi &gt; grid$pj) / pairs</code></pre>
<pre><code>## [1] 0.9910714</code></pre>
<pre class="r"><code># Percent Discordant
sum(grid$pi &lt; grid$pj) / pairs</code></pre>
<pre><code>## [1] 0.008928571</code></pre>
<pre class="r"><code># Somers&#39; D
nc &lt;- sum(grid$pi &gt; grid$pj)
(nc - (pairs - nc)) / pairs</code></pre>
<pre><code>## [1] 0.9821429</code></pre>
<pre class="r"><code># Tau-a 
2 * (nc - (pairs - nc)) / (nobs(fit) * (nobs(fit) - 1))</code></pre>
<pre><code>## [1] 0.5057471</code></pre>
</div>
<div id="parameter-estimates-and-confidence-intervals" class="section level3">
<h3>Parameter estimates and confidence intervals</h3>
<pre class="r"><code>summary(fit)$coefficients</code></pre>
<pre><code>##                 Estimate Std. Error    z value  Pr(&gt;|z|)
## (Intercept) -11.56273217 40.9868601 -0.2821083 0.7778605
## volatile      0.04500712  0.9463449  0.0475589 0.9620678
## credit        0.17384805  0.2632646  0.6603547 0.5090262</code></pre>
<p>The estimates of odds ratio is obtained by applying the exponential function to the parameter estimates and confidence intervals.</p>
<pre class="r"><code># profile likelihood confidence interval
confint(fit)</code></pre>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                   2.5 %    97.5 %
## (Intercept) -57.2306037        NA
## volatile             NA 0.8767002
## credit       -0.8975829 0.5591083</code></pre>
<pre class="r"><code>exp(cbind(OR = coef(fit)[-1], confint(fit)[-1, ]))  # odds ratio</code></pre>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                OR     2.5 %   97.5 %
## volatile 1.046035        NA 2.402957
## credit   1.189875 0.4075536 1.749112</code></pre>
<pre class="r"><code># Wald confidence intervals
confint.default(fit)</code></pre>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -91.8955018 68.7700374
## volatile     -1.8097948  1.8998090
## credit       -0.3421412  0.6898373</code></pre>
<pre class="r"><code>exp(coef(fit)[-1])  # odds ratio</code></pre>
<pre><code>## volatile   credit 
## 1.046035 1.189875</code></pre>
<pre class="r"><code>exp(confint.default(fit)[-1, ])</code></pre>
<pre><code>##              2.5 %   97.5 %
## volatile 0.1636877 6.684618
## credit   0.7102479 1.993391</code></pre>
</div>
<div id="classification-table" class="section level3">
<h3>Classification table</h3>
<p>SAS output have a nice classification table that shows the classification results under different cutoffs. Thanks to <a href="http://stats.stackexchange.com/a/4873">this answer</a>, R can achieve a similar table as well, though there is a little bit difference due to precision (the maximum absolute difference between the two predicted probabilities are 1.890711e-08).</p>
<pre class="r"><code>getMisclass &lt;- function(cutoff, p, event) {
   pred &lt;- factor(1*(p &gt; cutoff), labels = levels(event)) 
   t &lt;- table(pred, event)
   cat(&quot;cutoff &quot;, cutoff, &quot;:\n&quot;)
   print(t)
   cat(&quot;correct    :&quot;, round(sum(t[c(1,4)])/sum(t), 3),&quot;\n&quot;)
   cat(&quot;Specificity:&quot;, round(t[1]/sum(t[,1]), 3),&quot;\n&quot;)
   cat(&quot;Sensitivity:&quot;, round(t[4]/sum(t[,2]), 3),&quot;\n\n&quot;)
   invisible(t)
}
cutoffs &lt;- seq(0.1, .9, by=.1)
getMisclass(.5, p = fitted(fit), event = volatile$market)</code></pre>
<pre><code>## cutoff  0.5 :
##          event
## pred      emerge develop
##   emerge      15       1
##   develop      1      13
## correct    : 0.933 
## Specificity: 0.938 
## Sensitivity: 0.929</code></pre>
<p>You can apply this function to get a more thorough output like SAS.</p>
<pre class="r"><code>sapply(cutoffs, getMisclass, p=fitted(fit), event=volatile$market)</code></pre>
</div>
</div>
